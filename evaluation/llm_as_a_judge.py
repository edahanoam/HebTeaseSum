import pandas as pd
from openai import OpenAI

from multiling_data import load_datasets
from together import Together
from tqdm import tqdm
from rouge import Rouge
from evaluate import load
import numpy as np
import pandas as pd
import time
import re


def remove_phrases(row, phrase_dict, lang):
    summary = row['summary']
    if lang not in phrase_dict:
        print(lang)
        return summary

    phrases = phrase_dict[lang]
    for phrase in phrases:
        if pd.isna(phrase): continue
        summary = re.sub(re.escape(phrase), '', summary)
    return summary.strip()


def remove_pages():
    file_path = 'keywords.xlsx'

    xls = pd.read_excel(file_path, sheet_name=None, header=0)

    lang_to_phrases = {
        lang: df['k'].dropna().tolist()
        for lang, df in xls.items()
    }

    return lang_to_phrases


DEFAULT_MODEL = "gpt-4o-2024-08-06"


def build_star_rating_prompt(aspect, ant_aspect, aspect_ins, text, reference, generated):
    return f"""Score the following news summarization given the corresponding news with respect to {aspect} with one to five stars,
where one star means “{ant_aspect}” and five stars means “perfect {aspect}”.
Note that {aspect} measures {aspect_ins}.

{text}
Reference Summary: {reference}
Generated Summary: {generated}

Output only the number of stars (1 to 5). Do not include any explanation.
Stars:"""


def non_reference_based_prompt(text, generated):
    return f"""
    You will be provided with a document and a summary generated by a model.
    Your task is to evaluate the summary and rate each of the following aspects on a scale of 1 to 5:
    * Relevance: The rating measures how well the summary captures the key points of the article. Consider whether all and only the important aspects are contained in the summary.
    * Consistency: The rating measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.
    * Coherence: The rating measures the quality of all sentences collectively, do the fit together and sound naturally. Consider the quality of the summary as a whole.
     Rate each criterion on a scale of 1–5, where 1 = very poor and 5 = perfect. Please follow the JSON-style format below and no further explanations:
```json
{{
"relevance": int (1-5),
"consistency": int (1-5),
"coherence": int (1-5)
}}
```

    ** Document **
    {text}
    ** Summary **
    {generated}
    """


def build_reference(referece, generated):
    return f"You will receive a reference summary and a candidate summary. Your task is to compare these two summaries and assess the extent to which the candidate summary covers the information presented in the reference summary.\n Please indicate your agreement with the following statement: “All of the information in the reference summary can be found in the candidate summary.” \n Use the following 5-point scale when determining your response: \n 1. Strongly Disagree\n 2. Disagree\n 3. Neither Agree nor Disagree \n4. Agree\n 5. Strongly Agree \nInput: \nReference Summary:\n {referece} \nCandidate Summary: {generated} \nEvaluation Form (scores ONLY):\n - Agreement (1-5):"


def inference(client, fomattedcol, df, asp):
    inferences = []
    for i, row in tqdm(df.iterrows(), total=len(df)):
        current_prompt = [
            {"role": "user", "content": row[fomattedcol]}
        ]
        response = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=current_prompt,
            temperature=0
        )
        inferences.append(response.choices[0].message.content)
    time.sleep(6)
    df[f"star_{asp}"] = inferences
    return df


def prepare_aspects(df, name):
    relevance = ['relevance', 'irrelevance',
                 'how well the summary captures the key points of the article. Consider whether all and only the important aspects are contained in the summary']
    coherence = ['coherence', 'incoherence',
                 'the quality of all sentences collectively, to the fit together and sound naturally. Consider the quality of the summary as a whole']
    consistency = ['consistency', 'inconsistency',
                   'whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information']
    fluency = ['fluency', 'disfluency',
               'the quality of individual sentences, are they well-written and grammatically correct. Consider the quality of individual sentences']

    aspects = [relevance, coherence, consistency, fluency]
    for asp in aspects:
        aspect = asp[0]
        ant_aspect = asp[1]
        aspect_ins = asp[2]

        df[f'{aspect}_stars_prompt'] = df.apply(
            lambda row: build_star_rating_prompt(
                reference=row["summary"],
                generated=row["formatted_inference"],
                text=row["text"],
                aspect=aspect,
                ant_aspect=ant_aspect,
                aspect_ins=aspect_ins
            ),
            axis=1
        )
        client = OpenAI()
        df = inference(client, f'{aspect}_stars_prompt', df, aspect)

    df.to_csv(f"withllmjudge{name}")


def coherence_coverage(df, name):
    coherence = ['coherence', 'incoherence',
                 'the quality of all sentences collectively, to the fit together and sound naturally. Consider the quality of the summary as a whole']
    aspect = coherence[0]
    ant_aspect = coherence[1]
    aspect_ins = coherence[2]
    df[f'{aspect}_stars_prompt'] = df.apply(
        lambda row: build_star_rating_prompt(
            reference=row["summary_cleaned"],
            generated=row["formatted_inference"],
            text=row["text"],
            aspect=aspect,
            ant_aspect=ant_aspect,
            aspect_ins=aspect_ins
        ),
        axis=1
    )

    client = OpenAI()
    df = inference(client, f'{aspect}_stars_prompt', df, aspect)

    df['coverage_prompt'] = df.apply(
        lambda row: build_reference(row['summary_cleaned'], row['formatted_inference']),
        axis=1
    )
    df = inference(client, f'coverage_prompt', df, "coverage")

    df.to_csv(f"withllmjudge{name}")


def consistency(df, name):
    consistency = ['consistency', 'inconsistency',
                   'whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information']
    aspect = consistency[0]
    ant_aspect = consistency[1]
    aspect_ins = consistency[2]
    df[f'{aspect}_stars_prompt'] = df.apply(
        lambda row: build_star_rating_prompt(
            reference=row["summary_cleaned"],
            generated=row["formatted_inference"],
            text=row["text"],
            aspect=aspect,
            ant_aspect=ant_aspect,
            aspect_ins=aspect_ins
        ),
        axis=1
    )

    client = OpenAI()
    df = inference(client, f'{aspect}_stars_prompt', df, aspect)
    df.to_csv(f"withllmjudge_consitency{name}")


def non_reference_based(df, name):
    aspect = "no_ref"
    df[f'{aspect}_stars_prompt'] = df.apply(
        lambda row: non_reference_based_prompt(
            generated=row["formatted_inference"],
            text=row["text"],
        ),
        axis=1
    )

    client = OpenAI()
    df = inference(client, f'{aspect}_stars_prompt', df, aspect)
    df.to_csv(f"____ALL1109NOREFwithllmjudge{name}")


if __name__ == '__main__':
    bad_phrases = remove_pages()
    languages = ['Estonian', 'Greek', 'Italian', 'Icelandic', 'Norwegian', 'Polish', "Hebrew"]
    models = ["deepseek-ai", "meta-llama", "mistralai"]
    for language in languages:
        for model in models:
            temp_doc = f'combined_inference_{model}_{language}170524.csv'
            print(temp_doc)
            df = pd.read_csv(temp_doc)
            df['summary_cleaned'] = df.apply(lambda row: remove_phrases(row, bad_phrases, language), axis=1)
            non_reference_based(df, temp_doc)

